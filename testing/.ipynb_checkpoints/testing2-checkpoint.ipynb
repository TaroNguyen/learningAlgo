{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda3/envs/DeepLearning/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/Users/apple/anaconda3/envs/DeepLearning/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Build optmizer for training, using gradient clipping.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: Network loss\n",
    "        learning_rate: Learning rate for optimizer\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph=tf.Graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_dir = './graphs/'\n",
    "batch_size=128\n",
    "num_steps=365\n",
    "lstm_layers= 1\n",
    "lstm_size= 10\n",
    "inputs_dim= 1\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder( tf.float32, [batch_size,num_steps, inputs_dim] , name='inputs')\n",
    "    targets_ = tf.placeholder( tf.float32, [batch_size,num_steps, inputs_dim] , name='targets')\n",
    "    learning_rate= tf.placeholder( tf.float32 , name='lr')\n",
    "    grad_clip= tf.placeholder( tf.float32 , name='clip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'TensorShape' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-961fa3af5bfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'TensorShape' and 'int'"
     ]
    }
   ],
   "source": [
    "\n",
    "with graph.as_default():\n",
    "    rnn_cell = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    "\n",
    "# 'outputs' is a tensor of shape [batch_size, max_time, cell_state_size]\n",
    "\n",
    "# defining initial state\n",
    "    initial_state = rnn_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "\n",
    "# 'state' is a tensor of shape [batch_size, cell_state_size]\n",
    "    outputs, state = tf.nn.dynamic_rnn(rnn_cell, inputs_,\n",
    "                                   initial_state=initial_state,\n",
    "                                   dtype=tf.float32)\n",
    "    W = tf.Variable(tf.random_normal([lstm_size, inputs_dim]), dtype=tf.float32) # weights\n",
    "    b = tf.Variable(tf.random_normal([ inputs_dim]), dtype=tf.float32) # biases\n",
    "    shape = outputs.shape\n",
    "    \n",
    "    outputs = tf.matmul( tf.reshape( outputs, [-1, lstm_size]), W ) + b\n",
    "    outputs = tf.reshape( outputs, shape[:-1]+(inputs_dim,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    cost = tf.losses.mean_squared_error( outputs , targets_)\n",
    "    optimizer= build_optimizer(cost, learning_rate, grad_clip)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constant trend\n",
    "def batch_1(batch_size, num_steps):\n",
    "    trend = np.zeros( num_steps+1)\n",
    "    trend = np.reshape( trend, [-1,1])\n",
    "    while True:\n",
    "        batch = np.random.randn( batch_size,num_steps+1,1)\n",
    "        for i in range(batch_size):\n",
    "            batch[i,:,:]=batch[i,:,:]+trend[:,:]\n",
    "        \n",
    "        yield (batch[:,:-1,:] , batch[:,1:,:] )\n",
    "#sin trend\n",
    "def batch_2(batch_size, num_steps):\n",
    "    trend = np.arange( num_steps+1)\n",
    "    trend = np.sin(trend)\n",
    "    trend = np.reshape( trend, [-1,1])\n",
    "    while True:\n",
    "        batch = np.random.randn( batch_size,num_steps+1,1)\n",
    "        for i in range(batch_size):\n",
    "            batch[i,:,:]=batch[i,:,:]+trend[:,:]\n",
    "        \n",
    "        yield (batch[:,:-1,:] , batch[:,1:,:] )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator1 = batch_1(batch_size, num_steps )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator2 = batch_2(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator= generator2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(logs_dir, graph)\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(1000):\n",
    "        X,Y = next(generator)\n",
    "        state = sess.run( initial_state)\n",
    "        variables =tf.trainable_variables()\n",
    "        feed = { inputs_: X,\n",
    "                initial_state :state,\n",
    "               targets_: Y ,\n",
    "               learning_rate: 0.1,\n",
    "               grad_clip: 0.1}\n",
    "        sess.run( [optimizer], feed_dict=feed)\n",
    "        result = sess.run( [cost, outputs], feed_dict=feed)\n",
    "        if (i+1) %100 == 0 :\n",
    "            print( result[0]-1)\n",
    "        if (i+1)%1000 == 0 :\n",
    "            print( outputs[0,])\n",
    "writer.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
